{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrasterio\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mosgeo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ogr, gdal\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mshapely\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeometry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m polygon\n",
      "File \u001b[1;32mc:\\Users\\TCS\\anaconda3\\envs\\newtorchenv\\Lib\\site-packages\\torchvision\\__init__.py:5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodulefinder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Module\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Don't re-order these, we need to load the _C extension (done when importing\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# .extensions) before entering _meta_registrations.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\TCS\\anaconda3\\envs\\newtorchenv\\Lib\\site-packages\\torch\\__init__.py:2143\u001b[0m\n\u001b[0;32m   2141\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m library\n\u001b[0;32m   2142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m TYPE_CHECKING:\n\u001b[1;32m-> 2143\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations\n\u001b[0;32m   2145\u001b[0m \u001b[38;5;66;03m# Enable CUDA Sanitizer\u001b[39;00m\n\u001b[0;32m   2146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTORCH_CUDA_SANITIZER\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n",
      "File \u001b[1;32mc:\\Users\\TCS\\anaconda3\\envs\\newtorchenv\\Lib\\site-packages\\torch\\_meta_registrations.py:9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_prims_common\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SymBool, SymFloat, Tensor\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decomp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     _add_op_to_registry,\n\u001b[0;32m     11\u001b[0m     _convert_out_params,\n\u001b[0;32m     12\u001b[0m     global_decomposition_table,\n\u001b[0;32m     13\u001b[0m     meta_table,\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpOverload\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_prims\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _prim_elementwise_meta, ELEMENTWISE_PRIM_TYPE_PROMOTION_KIND\n",
      "File \u001b[1;32mc:\\Users\\TCS\\anaconda3\\envs\\newtorchenv\\Lib\\site-packages\\torch\\_decomp\\__init__.py:245\u001b[0m\n\u001b[0;32m    241\u001b[0m             decompositions\u001b[38;5;241m.\u001b[39mpop(op, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    244\u001b[0m \u001b[38;5;66;03m# populate the table\u001b[39;00m\n\u001b[1;32m--> 245\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decomp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecompositions\u001b[39;00m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_refs\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# See NOTE [Core ATen Ops]\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# list was copied from torch/_inductor/decomposition.py\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;66;03m# excluding decompositions that results in prim ops\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;66;03m# Resulting opset of decomposition is core aten ops\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\TCS\\anaconda3\\envs\\newtorchenv\\Lib\\site-packages\\torch\\_decomp\\decompositions.py:12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Callable, cast, Iterable, List, Optional, Tuple, Union\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_prims\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mprims\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_prims_common\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\TCS\\anaconda3\\envs\\newtorchenv\\Lib\\site-packages\\torch\\_prims\\__init__.py:791\u001b[0m\n\u001b[0;32m    773\u001b[0m imag \u001b[38;5;241m=\u001b[39m _make_prim(\n\u001b[0;32m    774\u001b[0m     schema\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimag(Tensor(a) self) -> Tensor(a)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    775\u001b[0m     meta\u001b[38;5;241m=\u001b[39mpartial(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    781\u001b[0m     doc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    782\u001b[0m )\n\u001b[0;32m    784\u001b[0m isfinite \u001b[38;5;241m=\u001b[39m _make_elementwise_unary_prim(\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124misfinite\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    786\u001b[0m     impl_aten\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39misfinite,\n\u001b[0;32m    787\u001b[0m     doc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    788\u001b[0m     type_promotion\u001b[38;5;241m=\u001b[39mELEMENTWISE_PRIM_TYPE_PROMOTION_KIND\u001b[38;5;241m.\u001b[39mALWAYS_BOOL,\n\u001b[0;32m    789\u001b[0m )\n\u001b[1;32m--> 791\u001b[0m lgamma \u001b[38;5;241m=\u001b[39m \u001b[43m_make_elementwise_unary_prim\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlgamma\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimpl_aten\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlgamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtype_promotion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mELEMENTWISE_PRIM_TYPE_PROMOTION_KIND\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDEFAULT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    798\u001b[0m log \u001b[38;5;241m=\u001b[39m _make_elementwise_unary_prim(\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    800\u001b[0m     impl_aten\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlog,\n\u001b[0;32m    801\u001b[0m     doc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    802\u001b[0m     type_promotion\u001b[38;5;241m=\u001b[39mELEMENTWISE_PRIM_TYPE_PROMOTION_KIND\u001b[38;5;241m.\u001b[39mDEFAULT,\n\u001b[0;32m    803\u001b[0m )\n\u001b[0;32m    805\u001b[0m log1p \u001b[38;5;241m=\u001b[39m _make_elementwise_unary_prim(\n\u001b[0;32m    806\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog1p\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    807\u001b[0m     impl_aten\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlog1p,\n\u001b[0;32m    808\u001b[0m     doc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    809\u001b[0m     type_promotion\u001b[38;5;241m=\u001b[39mELEMENTWISE_PRIM_TYPE_PROMOTION_KIND\u001b[38;5;241m.\u001b[39mDEFAULT,\n\u001b[0;32m    810\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\TCS\\anaconda3\\envs\\newtorchenv\\Lib\\site-packages\\torch\\_prims\\__init__.py:472\u001b[0m, in \u001b[0;36m_make_elementwise_unary_prim\u001b[1;34m(name, type_promotion, **kwargs)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_make_elementwise_unary_prim\u001b[39m(\n\u001b[0;32m    466\u001b[0m     name: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m, type_promotion: ELEMENTWISE_PRIM_TYPE_PROMOTION_KIND, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    467\u001b[0m ):\n\u001b[0;32m    468\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;124;03m    Creates an elementwise unary prim.\u001b[39;00m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 472\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_make_prim\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    473\u001b[0m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m(Tensor self) -> Tensor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_prim_elementwise_meta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtype_promotion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtype_promotion\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRETURN_TYPE\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNEW\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\TCS\\anaconda3\\envs\\newtorchenv\\Lib\\site-packages\\torch\\_prims\\__init__.py:320\u001b[0m, in \u001b[0;36m_make_prim\u001b[1;34m(schema, return_type, meta, impl_aten, doc, tags, use_old_custom_ops_api)\u001b[0m\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m arg\u001b[38;5;241m.\u001b[39malias_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m arg\u001b[38;5;241m.\u001b[39malias_info\u001b[38;5;241m.\u001b[39mis_write:\n\u001b[0;32m    319\u001b[0m             mutates_args\u001b[38;5;241m.\u001b[39mappend(arg\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m--> 320\u001b[0m     prim_def \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlibrary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcustom_op\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprims::\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_prim_impl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmutates_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmutates_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    326\u001b[0m     prim_def\u001b[38;5;241m.\u001b[39mregister_fake(meta)\n\u001b[0;32m    328\u001b[0m _prim_packet \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_ops\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mprims, name)\n",
      "File \u001b[1;32mc:\\Users\\TCS\\anaconda3\\envs\\newtorchenv\\Lib\\site-packages\\torch\\_library\\custom_ops.py:142\u001b[0m, in \u001b[0;36mcustom_op\u001b[1;34m(name, fn, mutates_args, device_types, schema)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner\n\u001b[1;32m--> 142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\TCS\\anaconda3\\envs\\newtorchenv\\Lib\\site-packages\\torch\\_library\\custom_ops.py:123\u001b[0m, in \u001b[0;36mcustom_op.<locals>.inner\u001b[1;34m(fn)\u001b[0m\n\u001b[0;32m    121\u001b[0m     schema_str \u001b[38;5;241m=\u001b[39m schema\n\u001b[0;32m    122\u001b[0m namespace, opname \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m::\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 123\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mCustomOpDef\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;66;03m# Check that schema's alias annotations match those of `mutates_args`.\u001b[39;00m\n\u001b[0;32m    126\u001b[0m     expected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\TCS\\anaconda3\\envs\\newtorchenv\\Lib\\site-packages\\torch\\_library\\custom_ops.py:168\u001b[0m, in \u001b[0;36mCustomOpDef.__init__\u001b[1;34m(self, namespace, name, schema, fn)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_context_fn: Optional[Callable] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_fn: Optional[Callable] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lib \u001b[38;5;241m=\u001b[39m \u001b[43mget_library_allowing_overwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_namespace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_register_to_dispatcher()\n\u001b[0;32m    170\u001b[0m OPDEFS[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qualname] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\TCS\\anaconda3\\envs\\newtorchenv\\Lib\\site-packages\\torch\\_library\\custom_ops.py:543\u001b[0m, in \u001b[0;36mget_library_allowing_overwrite\u001b[1;34m(namespace, name)\u001b[0m\n\u001b[0;32m    540\u001b[0m     OPDEF_TO_LIB[qualname]\u001b[38;5;241m.\u001b[39m_destroy()\n\u001b[0;32m    541\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m OPDEF_TO_LIB[qualname]\n\u001b[1;32m--> 543\u001b[0m lib \u001b[38;5;241m=\u001b[39m \u001b[43mlibrary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLibrary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFRAGMENT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    544\u001b[0m OPDEF_TO_LIB[qualname] \u001b[38;5;241m=\u001b[39m lib\n\u001b[0;32m    545\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m lib\n",
      "File \u001b[1;32mc:\\Users\\TCS\\anaconda3\\envs\\newtorchenv\\Lib\\site-packages\\torch\\library.py:69\u001b[0m, in \u001b[0;36mLibrary.__init__\u001b[1;34m(self, ns, kind, dispatch_key)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ns \u001b[38;5;129;01min\u001b[39;00m _reserved_namespaces \u001b[38;5;129;01mand\u001b[39;00m (kind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDEF\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m kind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFRAGMENT\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(ns, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is a reserved namespace. Please try creating a library with another name.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 69\u001b[0m frame \u001b[38;5;241m=\u001b[39m \u001b[43mtraceback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     70\u001b[0m filename, lineno \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39mfilename, frame\u001b[38;5;241m.\u001b[39mlineno\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm: Optional[Any] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_dispatch_library(kind, ns, dispatch_key, filename, lineno)\n",
      "File \u001b[1;32mc:\\Users\\TCS\\anaconda3\\envs\\newtorchenv\\Lib\\traceback.py:232\u001b[0m, in \u001b[0;36mextract_stack\u001b[1;34m(f, limit)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    231\u001b[0m     f \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39m_getframe()\u001b[38;5;241m.\u001b[39mf_back\n\u001b[1;32m--> 232\u001b[0m stack \u001b[38;5;241m=\u001b[39m \u001b[43mStackSummary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwalk_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    233\u001b[0m stack\u001b[38;5;241m.\u001b[39mreverse()\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stack\n",
      "File \u001b[1;32mc:\\Users\\TCS\\anaconda3\\envs\\newtorchenv\\Lib\\traceback.py:395\u001b[0m, in \u001b[0;36mStackSummary.extract\u001b[1;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f, lineno \u001b[38;5;129;01min\u001b[39;00m frame_gen:\n\u001b[0;32m    393\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m f, (lineno, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mklass\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_from_extended_frame_gen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextended_frame_gen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlookup_lines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlookup_lines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    397\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapture_locals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapture_locals\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\TCS\\anaconda3\\envs\\newtorchenv\\Lib\\traceback.py:434\u001b[0m, in \u001b[0;36mStackSummary._extract_from_extended_frame_gen\u001b[1;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[0;32m    430\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend(FrameSummary(\n\u001b[0;32m    431\u001b[0m         filename, lineno, name, lookup_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39mf_locals,\n\u001b[0;32m    432\u001b[0m         end_lineno\u001b[38;5;241m=\u001b[39mend_lineno, colno\u001b[38;5;241m=\u001b[39mcolno, end_colno\u001b[38;5;241m=\u001b[39mend_colno))\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m fnames:\n\u001b[1;32m--> 434\u001b[0m     \u001b[43mlinecache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckcache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;66;03m# If immediate lookup was desired, trigger lookups now.\u001b[39;00m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lookup_lines:\n",
      "File \u001b[1;32mc:\\Users\\TCS\\anaconda3\\envs\\newtorchenv\\Lib\\linecache.py:72\u001b[0m, in \u001b[0;36mcheckcache\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m   \u001b[38;5;66;03m# no-op for files loaded via a __loader__\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 72\u001b[0m     stat \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m     cache\u001b[38;5;241m.\u001b[39mpop(filename, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "\n",
    "import os\n",
    "import random\n",
    "from rasterio.features import shapes\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import rasterio\n",
    "\n",
    "from osgeo import ogr, gdal\n",
    "from torchvision.transforms import transforms\n",
    "from tqdm import tqdm\n",
    "from shapely.geometry import polygon\n",
    "from utils import generate_patch_coordinates\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = os.path.join('C:','Users','bhatt','OneDrive','Desktop','butwal data')\n",
    "image_dir = \"C:\\\\Users\\\\bhatt\\\\OneDrive\\\\Desktop\\\\butwal data\\\\Orthomosaics Output\\\\Butwal Orthomosaics Only\"\n",
    "mask_dir = \"C:\\\\Users\\\\bhatt\\\\OneDrive\\\\Documents\\\\ArcGIS\\\\Projects\\\\MyProject33\"\n",
    "current_image_file_name = 'Mission 2.tif'\n",
    "current_mask_file_name = 'Buildings.shp'\n",
    "\n",
    "# Path to the image file in .tif format\n",
    "image_path = os.path.join(image_dir, current_image_file_name)\n",
    "# Path to the shape file containing the landcover classes in .shp format\n",
    "mask_path = os.path.join(mask_dir,\n",
    "                         current_mask_file_name)\n",
    "# Path to the shape file containing the landcover classes in .shp format\n",
    "# mask_path = os.path.join(mask_dir,\n",
    "#                          current_mask_file_name)\n",
    "output12_dir = os.path.join('.', 'output12')\n",
    "os.makedirs(output12_dir, exist_ok=True)\n",
    "\n",
    "rasterized_dir = os.path.join(output12_dir, 'rasterized_outputs')\n",
    "os.makedirs(rasterized_dir, exist_ok=True)\n",
    "# rasterized_dir = os.path.join(output_dir, 'rasterized_outputs')\n",
    "# os.makedirs(rasterized_dir, exist_ok=True)\n",
    "rasterized_file_name = \"Result12.tif\"\n",
    "rasterized_file_path = os.path.join(rasterized_dir, rasterized_file_name)\n",
    "patch_size = 513\n",
    "stride = 256\n",
    "boundary = [3053710.461731, 751158.377075, 3055429.699707, 753175.112122]\n",
    "patch_output_dir = os.path.join(output12_dir, f\"{patch_size}x{patch_size}\")\n",
    "os.makedirs(patch_output_dir, exist_ok=True)\n",
    "labels_output_dir = os.path.join(patch_output_dir, 'labels')\n",
    "os.makedirs(labels_output_dir, exist_ok=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_shape_file_path = mask_path\n",
    "image_file_path = image_path\n",
    "\n",
    "gdalformat = 'GTiff'\n",
    "datatype = gdal.GDT_Float32\n",
    "\n",
    "# Get projection info from reference image\n",
    "Image = gdal.Open(image_file_path, gdal.GA_ReadOnly)\n",
    "\n",
    "# Open Shapefile\n",
    "Shapefile = ogr.Open(mask_shape_file_path)\n",
    "Shapefile_layer = Shapefile.GetLayer()\n",
    "\n",
    "# Rasterise\n",
    "print(\"Rasterizing shapefile...\")\n",
    "\n",
    "Output = gdal.GetDriverByName(gdalformat).Create(rasterized_file_path, Image.RasterXSize, Image.RasterYSize, 1,\n",
    "                                                 datatype,\n",
    "                                                 options=['COMPRESS=DEFLATE'])\n",
    "Output.SetProjection(Image.GetProjectionRef())\n",
    "Output.SetGeoTransform(Image.GetGeoTransform())\n",
    "\n",
    "# Write data to band 1\n",
    "Band = Output.GetRasterBand(1)\n",
    "Band.SetNoDataValue(0)\n",
    "\n",
    "gdal.RasterizeLayer(Output, [1], Shapefile_layer, options=['ATTRIBUTE=Class_id'])\n",
    "\n",
    "# Close datasets\n",
    "Band = None\n",
    "Output = None\n",
    "Image = None\n",
    "Shapefile = None\n",
    "\n",
    "print(\"Rasterization of shapefile completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the rasters\n",
    "with rasterio.open(image_path) as raster1:\n",
    "    temp_img = raster1.read()  # Read raster1 into numpy array\n",
    "\n",
    "with rasterio.open(rasterized_file_path) as raster2:\n",
    "    temp_mask = raster2.read(1)  # Read raster2 into numpy array\n",
    "\n",
    "labels, count = np.unique(temp_mask, return_counts=True)  #Check for each channel. All channels are identical\n",
    "print('Unique values in mask: ', labels)\n",
    "print('Counts of unique values: ', count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(rasterized_file_path, \"r\") as src:\n",
    "    mask_data = src.read()\n",
    "    mask_meta = src.meta\n",
    "\n",
    "print(np.unique(mask_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_patches\n",
    "# image_path=\"C:\\\\Users\\\\bhatt\\\\OneDrive\\Desktop\\\\butwal data\\\\Orthomosaics Output\\\\Butwal Orthomosaics Only\\\\Mission 1.tif\"\n",
    "# rasterized_file_path=\"C:\\\\Users\\\\bhatt\\\\OneDrive\\\\Desktop\\\\semprj\\\\aerial-satellite-imagery-segmentation-nepal\\\\output11\\\\rasterized_outputs\\\\Result11.tif\"\n",
    "# patch_size= 513\n",
    "# stride=256\n",
    "# patch_output_dir=\"C:\\\\Users\\\\bhatt\\\\OneDrive\\\\Desktop\\\\semprj\\\\aerial-satellite-imagery-segmentation-nepal\\\\output11\\\\513x513\"\n",
    "create_patches(image_path, rasterized_file_path, patch_output_dir, patch_size, stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patch_output_dir=r\"C:\\Users\\bhatt\\OneDrive\\Desktop\\semprj\\aerial-satellite-imagery-segmentation-nepal\\output12\\513x513\"\n",
    "image_files = [f for f in os.listdir(patch_output_dir + \"\\\\images\") if f.endswith('.jpg')]\n",
    "\n",
    "# Select a random image file\n",
    "random_image_file = random.choice(image_files)\n",
    "\n",
    "# Construct the full paths to the image and mask files\n",
    "temp_image_path = os.path.join(patch_output_dir, \"images\", random_image_file)\n",
    "temp_mask_path = os.path.join(patch_output_dir, \"masks\", random_image_file.replace('.jpg', '_mask.jpg'))\n",
    "\n",
    "# Open the image and mask files using Rasterio\n",
    "with rasterio.open(temp_image_path) as src:\n",
    "    image = src.read().astype(float)\n",
    "    image_transform = src.transform\n",
    "\n",
    "with rasterio.open(temp_mask_path) as src:\n",
    "    mask = src.read()\n",
    "    mask_transform = src.transform\n",
    "\n",
    "# The image data read by Rasterio is in (bands, rows, cols) order\n",
    "# Convert the image data to (rows, cols, bands) order for visualization\n",
    "transposed_image = image.transpose((1, 2, 0))\n",
    "\n",
    "if 0 not in np.unique(transposed_image):\n",
    "    # Scale the image data to be between 0 and 1 for better visualization\n",
    "    transposed_image -= transposed_image.min()\n",
    "    transposed_image /= transposed_image.max()\n",
    "\n",
    "    # Create plots\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15, 15))\n",
    "\n",
    "    # Display the image\n",
    "    ax[0].imshow(transposed_image)\n",
    "    ax[0].set_title(\"Image\")\n",
    "\n",
    "    # Display the mask\n",
    "    transposed_mask = mask.transpose((1, 2, 0))\n",
    "\n",
    "    ax[1].imshow(transposed_mask)\n",
    "    ax[1].set_title(\"Mask\")\n",
    "    print(np.unique(mask))\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    print('Image shape: ', image.shape)\n",
    "    print('Mask shape: ', mask.shape)\n",
    "else:\n",
    "    print(\"Blank data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import rasterio\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Get list of image files\n",
    "image_files = [f for f in os.listdir(patch_output_dir + \"\\\\images\") if f.endswith('.jpg')]\n",
    "print(image_files)\n",
    "# Select a random image file\n",
    "random_image_file = random.choice(image_files)\n",
    "\n",
    "# Construct the full paths to the image and mask files\n",
    "temp_image_path = os.path.join(patch_output_dir, \"images\", random_image_file)\n",
    "temp_mask_path = os.path.join(patch_output_dir, \"masks\", random_image_file.replace('.jpg', '_mask.jpg'))\n",
    "\n",
    "# Open the image and mask files using Rasterio\n",
    "with rasterio.open(temp_image_path) as src:\n",
    "    image = src.read().astype(float)\n",
    "    image_transform = src.transform\n",
    "# temp_mask_path=r\"C:\\Users\\bhatt\\OneDrive\\Desktop\\semprj\\aerial-satellite-imagery-segmentation-nepal\\Output_mg\\513x513\\masks\\test\\Mission4patch_955_mask.jpg\"\n",
    "\n",
    "with rasterio.open(temp_mask_path) as src:\n",
    "    mask = src.read()\n",
    "    mask_transform = src.transform\n",
    "print(np.unique(mask))\n",
    "# The image data read by Rasterio is in (bands, rows, cols) order\n",
    "# Convert the image data to (rows, cols, bands) order for visualization\n",
    "transposed_image = image.transpose((1, 2, 0))\n",
    "\n",
    "# Scale the image data to be between 0 and 1 for better visualization\n",
    "transposed_image -= transposed_image.min()\n",
    "transposed_image /= transposed_image.max()\n",
    "\n",
    "# Create plots\n",
    "fig, ax = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Display the image\n",
    "ax[0].imshow(transposed_image)\n",
    "ax[0].set_title(\"Image\")\n",
    "\n",
    "# Display the first channel of the mask\n",
    "ax[1].imshow(mask[0,:,:], cmap='gray')\n",
    "ax[1].set_title(\"Mask Channel 1\")\n",
    "\n",
    "# Display the second channel of the mask\n",
    "ax[2].imshow(mask[1,:,:], cmap='gray')\n",
    "ax[2].set_title(\"Mask Channel 2\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# Create plots\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "\n",
    "# # Combine the channels into a single image\n",
    "# combined_mask = np.stack((mask[0,:,:], mask[1,:,:], np.zeros_like(mask[0,:,:])), axis=-1)\n",
    "\n",
    "# # Display the combined mask\n",
    "# ax.imshow(combined_mask)\n",
    "# ax.set_title(\"Combined Mask\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "print('Image shape:', image.shape)\n",
    "print('Mask shape:', mask.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(patch_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generator2 import NepalDataGenerator\n",
    "from generator2 import NepalDataset\n",
    "from utils import visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = patch_output_dir\n",
    "in_channels = 4\n",
    "num_classes = 4\n",
    "batch_size = 4\n",
    "shuffle = True\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    \n",
    "])\n",
    "\n",
    "dataset = NepalDataset(data_path, transform=transform)\n",
    "print(f\"Dataset size: {format(len(dataset))}\")\n",
    "data_generator = NepalDataGenerator(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "print(f\"Data generator size: {format(len(data_generator))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_images_to_show = 4 # Number of images to show from the batch\n",
    "\n",
    "images, masks = data_generator.__next__()  # Get the batch of images and masks\n",
    "\n",
    "if no_of_images_to_show > batch_size:\n",
    "    no_of_images_to_show = batch_size\n",
    "\n",
    "for i in range(0, no_of_images_to_show):\n",
    "    image = images[i].permute(1, 2, 0).numpy()  # Access individual image and convert to numpy array\n",
    "    mask = masks[i].squeeze().numpy()  # Access individual mask and convert to numpy array\n",
    "    visualize(image, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOLO V8 preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from shapely.geometry import Polygon\n",
    "import tifffile\n",
    "\n",
    "\n",
    "def mask_to_polygons(img_path, mask_path):\n",
    "    '''\n",
    "    Convierte una máscara de imagen en polígonos. Devuelve dos listas:\n",
    "    - Lista de polígonos de shapely sin normalizar\n",
    "    - Lista de polígonos de shapely normalizados (coordenadas entre 0 y 1)\n",
    "\n",
    "    Args:\n",
    "        img_path (str): Ruta al archivo de imagen original.\n",
    "        mask_path (str): Ruta al archivo de la máscara en escala de grises.\n",
    "    '''\n",
    "\n",
    "    mask =tifffile.imread(mask_path)\n",
    "\n",
    "    # mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    # Calcula los contornos \n",
    "    mask = mask.astype(bool)\n",
    "    #contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # convertimos los contornos a polígonos de Label Studio\n",
    "    polygons = []\n",
    "    normalized_polygons = []\n",
    "    for contour in contours:\n",
    "\n",
    "        # Lo meto en un try porque la extraccion de polígonos que hace el opencv a partir de la máscara\n",
    "        # a veces genera polígonos de menos de 4 vértices, que no tiene sentido por no ser cerrados, \n",
    "        # provocando que falle al convertir a polígno de shapely\n",
    "\n",
    "        try:\n",
    "            polygon = contour.reshape(-1, 2).tolist()\n",
    "\n",
    "            # normalizamos las coordenadas entre 0 y 1 porque así lo requiere YOLOv8\n",
    "            normalized_polygon = [[round(coord[0] / mask.shape[1], 4), round(coord[1] / mask.shape[0], 4)] for coord in\n",
    "                                  polygon]\n",
    "\n",
    "            # Convertimos a objeto poligono de shapely (sin normalizar)\n",
    "            polygon_shapely = Polygon(polygon)\n",
    "            simplified_polygon = polygon_shapely.simplify(0.85, preserve_topology=True)\n",
    "            polygons.append(simplified_polygon)\n",
    "\n",
    "            # normalizdos\n",
    "            normalized_polygons.append(Polygon(normalized_polygon))\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    return polygons, normalized_polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_to_polygons_per_class(img_path, mask_path):\n",
    "    # Map grayscale mask values to your class indices\n",
    "    class_mapping = {0: 0, 1: 1,2:2}\n",
    "\n",
    "    mask = tifffile.imread(mask_path)\n",
    "\n",
    "    polygons_per_class = {}\n",
    "    for mask_value in np.unique(mask):\n",
    "        # Look up class index using mask value, if no mapping is found then continue\n",
    "        class_index = class_mapping.get(mask_value)\n",
    "        if class_index is None:\n",
    "            continue\n",
    "\n",
    "        class_mask = np.where(mask == mask_value, 1, 0).astype(np.uint8)\n",
    "\n",
    "        contours, _ = cv2.findContours(class_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "\n",
    "        polygons = []\n",
    "        for contour in contours:\n",
    "            try:\n",
    "                polygon = contour.reshape(-1, 2).tolist()\n",
    "\n",
    "                normalized_polygon = [\n",
    "                    \n",
    "                    [round(coord[0] / mask.shape[1], 4), round(coord[1] / mask.shape[0], 4)]\n",
    "                    for coord in polygon\n",
    "                ]\n",
    "\n",
    "                if class_index not in polygons_per_class:\n",
    "                    polygons_per_class[class_index] = []\n",
    "                polygons_per_class[class_index].append(Polygon(normalized_polygon))\n",
    "\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "    return polygons_per_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "input_dir = os.path.join(patch_output_dir, 'masks')\n",
    "output_dir = labels_output_dir\n",
    "\n",
    "for j in os.listdir(input_dir):\n",
    "    image_path = os.path.join(input_dir, j)\n",
    "    polygons, normalized_polygons = mask_to_polygons(image_path, image_path)  # Separate lists for each class\n",
    "\n",
    "    # print the polygons\n",
    "    file_name_without_ext = os.path.splitext(j)[0]\n",
    "    if file_name_without_ext.endswith('_mask'):\n",
    "        file_name_without_ext = file_name_without_ext[:-5]\n",
    "\n",
    "    with open('{}.txt'.format(os.path.join(output_dir, file_name_without_ext)), 'w') as f:\n",
    "        for class_label in range(3):\n",
    "            for polygon in normalized_polygons:\n",
    "                f.write('{} '.format(class_label))  # Add class label at the beginning of each line\n",
    "                if isinstance(polygon, Polygon):\n",
    "                    x, y = polygon.exterior.xy\n",
    "                    for i in range(len(x)):\n",
    "                        f.write('{} {}\\t'.format(x[i], y[i]))\n",
    "                f.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "input_dir = os.path.join(patch_output_dir, 'masks')\n",
    "output_dir = labels_output_dir\n",
    "\n",
    "\n",
    "for j in os.listdir(input_dir):\n",
    "    image_path = os.path.join(input_dir, j)\n",
    "    polygons_per_class = mask_to_polygons_per_class(image_path, image_path)\n",
    "\n",
    "    # print the polygons\n",
    "    file_name_without_ext = os.path.splitext(j)[0]\n",
    "    if file_name_without_ext.endswith('_mask'):\n",
    "        file_name_without_ext = file_name_without_ext[:-5]\n",
    "    \n",
    "    with open('{}.txt'.format(os.path.join(output_dir, file_name_without_ext)), 'w') as f:\n",
    "        for class_label, polygons in polygons_per_class.items():\n",
    "            for polygon in polygons:\n",
    "                f.write('{} '.format(class_label))  # Add class label at the beginning of each line\n",
    "                x, y = polygon.exterior.xy\n",
    "                for i in range(len(x)):\n",
    "                    f.write('{} {}\\t'.format(x[i], y[i]))\n",
    "                f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import shutil\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# def split_dataset_into_train_val_test(input_folder, output_folder, ratio=(0.7, 0.15, 0.15)):\n",
    "#     assert sum(ratio) == 1, \"Ratios must add up to 1.\"\n",
    "\n",
    "#     # Get all file names in the input folder\n",
    "#     all_files = os.listdir(input_folder)\n",
    "#     np.random.shuffle(all_files)\n",
    "\n",
    "#     num_files = len(all_files)\n",
    "#     train_files = all_files[:int(num_files * ratio[0])]\n",
    "#     val_files = all_files[int(num_files * ratio[0]):int(num_files * (ratio[0] + ratio[1]))]\n",
    "#     test_files = all_files[int(num_files * (ratio[0] + ratio[1])):]\n",
    "\n",
    "#     # Create output directories\n",
    "#     for dir in ['train', 'val', 'test']:\n",
    "#         os.makedirs(os.path.join(output_folder, dir), exist_ok=True)\n",
    "\n",
    "#     # Move files to respective directories\n",
    "#     for file in train_files:\n",
    "#         shutil.move(os.path.join(input_folder, file), os.path.join(output_folder, 'train', file))\n",
    "\n",
    "#     for file in val_files:\n",
    "#         shutil.move(os.path.join(input_folder, file), os.path.join(output_folder, 'val', file))\n",
    "\n",
    "#     for file in test_files:\n",
    "#         shutil.move(os.path.join(input_folder, file), os.path.join(output_folder, 'test', file))\n",
    "\n",
    "\n",
    "# directories = ['images', 'labels', 'masks']\n",
    "# patch_output_directory = patch_output_dir\n",
    "\n",
    "# for directory in directories:\n",
    "#     input_folder = os.path.join(patch_output_directory, directory)\n",
    "#     output_folder = os.path.join(patch_output_directory, directory)\n",
    "#     split_dataset_into_train_val_test(input_folder, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "def split_dataset_into_train_val_test(input_folder_images, input_folder_masks, output_folder, ratio=(0.7, 0.15, 0.15)):\n",
    "    assert sum(ratio) == 1, \"Ratios must add up to 1.\"\n",
    "    # Get all image file names in the input folder (tif)\n",
    "    all_image_files = [f for f in os.listdir(input_folder_images) if f.endswith('.tif')]\n",
    "    np.random.shuffle(all_image_files)\n",
    "    num_files = len(all_image_files)\n",
    "    train_files = all_image_files[:int(num_files * ratio[0])]\n",
    "    val_files = all_image_files[int(num_files * ratio[0]):int(num_files * (ratio[0] + ratio[1]))]\n",
    "    test_files = all_image_files[int(num_files * (ratio[0] + ratio[1])):]\n",
    "    # Create output directories\n",
    "    for dir in ['train', 'val', 'test']:\n",
    "        os.makedirs(os.path.join(output_folder, 'images', dir), exist_ok=True)\n",
    "        os.makedirs(os.path.join(output_folder, 'masks', dir), exist_ok=True)\n",
    "    # Function to move files\n",
    "    def move_files(files, subset):\n",
    "        for file in files:\n",
    "            base_name = os.path.splitext(file)[0]\n",
    "            image_file = file\n",
    "            mask_file = base_name + '_mask.tif'  # Updated for tif\n",
    "            src_image_path = os.path.join(input_folder_images, image_file)\n",
    "            src_mask_path = os.path.join(input_folder_masks, mask_file)\n",
    "            dest_image_path = os.path.join(output_folder, 'images', subset, image_file)\n",
    "            dest_mask_path = os.path.join(output_folder, 'masks', subset, mask_file)\n",
    "            # Check if the mask file exists before moving\n",
    "            if os.path.exists(src_mask_path):\n",
    "                shutil.move(src_image_path, dest_image_path)\n",
    "                shutil.move(src_mask_path, dest_mask_path)\n",
    "            else:\n",
    "                print(f\"Warning: Mask file {src_mask_path} not found for image file {src_image_path}. Skipping.\")\n",
    "    move_files(train_files, 'train')\n",
    "    move_files(val_files, 'val')\n",
    "    move_files(test_files, 'test')\n",
    "# Directories\n",
    "patch_output_directory = patch_output_dir  # your patches folder\n",
    "input_folder_images = os.path.join(patch_output_directory, 'images')\n",
    "input_folder_masks = os.path.join(patch_output_directory, 'masks')\n",
    "split_dataset_into_train_val_test(input_folder_images, input_folder_masks, patch_output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# import os\n",
    "\n",
    "\n",
    "# def merge_all_files(directory):\n",
    "#     main_folders = ['images', 'labels', 'masks']\n",
    "#     subfolders = ['test', 'train', 'val']\n",
    "\n",
    "#     for main_folder in main_folders:\n",
    "#         main_folder_path = os.path.join(directory, main_folder)\n",
    "\n",
    "#         for subfolder in subfolders:\n",
    "#             subfolder_path = os.path.join(main_folder_path, subfolder)\n",
    "\n",
    "#             # merge all files in subfolder to main_folder\n",
    "#             for filename in os.listdir(subfolder_path):\n",
    "#                 shutil.move(os.path.join(subfolder_path, filename), main_folder_path)\n",
    "\n",
    "#             # remove subfolder\n",
    "#             shutil.rmtree(subfolder_path)\n",
    "# # function call\n",
    "# merge_all_files(patch_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('yolov8m-seg.pt')\n",
    "\n",
    "model.train(data='config.yaml', epochs=100, imgsz=512, single_cls=False,patience=30,optimizer='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics=model.val()\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model_path = \"C:\\\\Users\\\\bhatt\\\\runs\\\\segment\\\\train10\\\\weights\\\\best.pt\"\n",
    "\n",
    "image_path = \"C:\\\\Users\\\\bhatt\\\\OneDrive\\\\Desktop\\\\semprj\\\\aerial-satellite-imagery-segmentation-nepal\\\\output\\\\512x512\\\\images\\\\test\\\\patch_1700.jpg\"\n",
    "\n",
    "img= cv2.imread(image_path)\n",
    "H, W, _ = img.shape\n",
    "\n",
    "model = YOLO(model_path)\n",
    "\n",
    "results = model(img)\n",
    "\n",
    "# Process results list\n",
    "for result in results:\n",
    "    boxes = result.boxes  # Boxes object for bounding box outputs\n",
    "    masks = result.masks  # Masks object for segmentation masks outputs\n",
    "    keypoints = result.keypoints  # Keypoints object for pose outputs\n",
    "    probs = result.probs  # Probs object for classification outputs\n",
    "    result.show()  # display to screen\n",
    "    result.save(filename='result.jpg')  # save to disk\n",
    "# \n",
    "# for result in results:\n",
    "#     for j,mask in enumerate(result.masks.data):\n",
    "#         mask=mask.cpu().numpy()*255\n",
    "#         mask=cv2.resize(mask, (W,H))\n",
    "#         visualize(img,mask)\n",
    "#         print(f\"Class of mask {j}: {result.masks.data.names[j]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import matplotlib.pyplot as plt\n",
    "# import tifffile\n",
    "# patch_output_dir = r'output\\512x512'\n",
    "# input_dir = os.path.join(patch_output_dir, 'masks')\n",
    "# # output_dir = labels_output_dir\n",
    "\n",
    "# for j in os.listdir(input_dir):\n",
    "#     print(j)\n",
    "#     image_path = os.path.join(input_dir, j)\n",
    "#     # mask = tifffile.imread(image_path)\n",
    "#     plt.imread(r\"output\\512x512\\masks\\test\\patch_381_mask.jpg\")\n",
    "#     plt.show()\n",
    "#     # image_path = os.path.join(input_dir, j)\n",
    "#     # polygons, normalized_polygons = mask_to_polygons(image_path, image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newtorchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
